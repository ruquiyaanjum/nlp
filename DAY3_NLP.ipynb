{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb155778-e7d6-4435-8767-1384c2de8b4b",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762e2d8-321f-43fc-82dc-8a401ef3b081",
   "metadata": {},
   "source": [
    "text to number / vectors (Numerical representation)\n",
    "\n",
    "input --> I  love NLP --> 21  44   55 --> MODEL (ML / DL) --> output (+ve / -ve) 0 or 1\n",
    "\n",
    "Bag of Words:\n",
    "    * binary BOW\n",
    "    * sent1. He is intelligent  boy\n",
    "      sent2. She is intelligent  girl\n",
    "      sent3. Both of them are intelligent boy and girl\n",
    "      --> remove stopwords & lowering:\n",
    "                  sent1. intelligent  boy\n",
    "                  sent2. intelligent  girl\n",
    "                  sent3. intelligent boy and girl\n",
    "    \n",
    "\n",
    "    * Text Processing:\n",
    "           $ He is intelligent !# boy  \n",
    "           step1 --> Data Cleaning\n",
    "           He is intelligent boy\n",
    "           \n",
    "           step2 --> Lowercase\n",
    "           he is intelligent boy\n",
    "           \n",
    "           step3 --> remove stopwords\n",
    "           intelligent boy\n",
    "           \n",
    "           step4 --> stemming / lemmatization\n",
    "           step5 --> tokenization  (token id)\n",
    "           \n",
    "    * vocabulary : 20k words (dictionary) --> 1...20k --> A...z .\n",
    "      he love nlp --> 256  55  45 (based on the dictionary (index number))-> is called Token ID\n",
    "      \n",
    "      he love nlp and he love nlp and ml -->  \n",
    "      ml is not present in dictionary  --> create our own dictionary (unique words) \n",
    "      \n",
    "    * for implementing BOW :\n",
    "       step1: Data Cleaning\n",
    "       step2: Lowering\n",
    "       step3: remove stopwords\n",
    "       step4: stemming\n",
    "       step5: calculate histogram             (retrive important information).\n",
    "       \n",
    "      \n",
    "    * histogram (finding the freuency of words):\n",
    "      \n",
    "      words          frequency\n",
    "      intelligent       3\n",
    "      boy               2\n",
    "      girl              2\n",
    "      \n",
    "    * feature set | feature engineering:\n",
    "    \n",
    "      s.no    intelligent    boy   girl    y/output\n",
    "      sent1.      1           1     0\n",
    "      sent2.      1           0     1\n",
    "      sent3.      1           1     1             ----> (training data)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306e505a-85ae-4c71-a0f8-8a107ec85b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   He love   NLP  \n"
     ]
    }
   ],
   "source": [
    "#DATA CLEANING\n",
    "sentence1 = \"!@ He love % NLP !\"\n",
    "import re #regular expression or regex\n",
    "#print(re.sub(r\"[!@%#]\",\"\",sentence1))\n",
    "print(re.sub(r\"[^a-zA-Z]\",\" \",sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a27f7ca-7f85-4be2-8529-29f5cc7095b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is sunny outside\n"
     ]
    }
   ],
   "source": [
    "#how to use regex\n",
    "import re\n",
    "sentence2 = \"It is raining outside\"\n",
    "print(re.sub(r\"raining\",\"sunny\",sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9c69b-157a-4c74-8a29-c927dce182a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dataset is small bag of words works good , for large dataset may not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ed472b-71e3-4b4c-ac6e-d21a1bfe6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
    "               the world have come and invaded us, captured our lands, conquered our minds.\n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
    "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
    "               We have not grabbed their land, their culture,\n",
    "               their history and tried to enforce our way of life on them.\n",
    "               Why? Because we respect the freedom of others.That is why my\n",
    "               first vision is that of freedom. I believe that India got its first vision of\n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e480cc82-a4ba-42d0-9f52-61f8b28640e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5bebb2-b8bf-46f7-87e5-2b8b26d70c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()\n",
    "corpus = []\n",
    "############ DATA PROCESSING ############ \n",
    "for i in range(len(sentences)):\n",
    "    new_sent = re.sub(r\"[^a-zA-Z]\",\" \",sentences[i])   #data cleaning step\n",
    "    new_sent = new_sent.lower()         #lowering\n",
    "    new_sent = new_sent.split()    # converting to words\n",
    "    #to remove stopwords\n",
    "    new_sent = [stemmer.stem(word) for word in new_sent if word not in set(nltk.corpus.stopwords.words(\"english\"))]\n",
    "    \n",
    "    new_sent = \" \".join(new_sent)\n",
    "    corpus.append(new_sent)\n",
    "#print(corpus)  \n",
    "########### DATA IS READY ############## CONVERT TEXT TO NUMBERS #############\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2000, stop_words=\"english\", lowercase=True)\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c570d3c-9109-410f-8970-b2bee5fec1ad",
   "metadata": {},
   "source": [
    "         disadvantages                               Advantages\n",
    "\n",
    "         1 semantic information (x)              easy to implement\n",
    "         2 context (x)                           processing time is less\n",
    "         3 word weightage (x)                    works well with smaller dataset\n",
    "         4 bigger dataset BOW does not,\n",
    "           work well.\n",
    "         5 sparsity (sparse matrix)\n",
    "         6 order of words &  context missing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
